package ohnosequences.mg7

import ohnosequences.mg7.loquats._
import ohnosequences.loquat._
import ohnosequences.datasets._
import ohnosequences.awstools.s3._
import com.amazonaws.auth._, profile._


trait AnyMG7Pipeline {

  val inputSamples: Map[SampleID, S3Resource]
  val outputS3Folder: (SampleID, StepName) => S3Folder

  type Parameters <: AnyMG7Parameters
  val  parameters: Parameters

  val splitConfig:  AnySplitConfig
  val blastConfig:  AnyBlastConfig
  val assignConfig: AnyAssignConfig
  val mergeConfig:  AnyMergeConfig
  val countConfig:  AnyCountConfig


  // Boilerplate definitions that are derived from the ones above:

  case object dataMappings {

    lazy val split = inputSamples.toList.map { case (sampleId, readsS3Resource) =>

      DataMapping(sampleId, splitDataProcessing(parameters))(
        remoteInput = Map(
          data.mergedReads -> readsS3Resource
        ),
        remoteOutput = Map(
          data.fastaChunks -> S3Resource(outputS3Folder(sampleId, "split"))
        )
      )
    }

    private def instanceS3client(): S3 = S3.create(
      new AWSCredentialsProviderChain(
        new InstanceProfileCredentialsProvider(),
        new ProfileCredentialsProvider()
      )
    )

    private def listChunks(s3prefix: AnyS3Address): List[S3Object] = {
      instanceS3client().listObjects(s3prefix.bucket, s3prefix.key)
    }

    // These prefixes will be used several times, so they factored in methods:
    private def blastChunksS3Prefix(sampleId: String): S3Folder = outputS3Folder(sampleId, "blast") / "chunks" /
    private def blastNoHitsS3Prefix(sampleId: String): S3Folder = outputS3Folder(sampleId, "blast") / "no-hits" /

    // Here we generate tasks/data mappings for the blast loquat one per each S3 object generated by the split loquat
    lazy val blast = split.flatMap { splitDM =>
      val sampleId = splitDM.label

      listChunks( splitDM.remoteOutput(data.fastaChunks).resource )
        .zipWithIndex
        .map { case (chunkS3Obj, n) =>

          DataMapping(s"${sampleId}.${n}", blastDataProcessing(parameters))(
            remoteInput = Map(
              data.fastaChunk -> S3Resource(chunkS3Obj)
            ),
            remoteOutput = Map(
              data.blastChunk  -> S3Resource(blastChunksS3Prefix(sampleId) / s"blast.${n}.csv"),
              data.noHitsChunk -> S3Resource(blastNoHitsS3Prefix(sampleId) / s"no-hits.${n}.fa")
            )
          )
        }
    }

    // These prefixes will be used several times, so they factored in methods:
    private def lcaAssignS3Prefix(sampleId: String): S3Folder = outputS3Folder(sampleId, "assign") / "lca" /
    private def bbhAssignS3Prefix(sampleId: String): S3Folder = outputS3Folder(sampleId, "assign") / "bbh" /

    lazy val assign = inputSamples.keys.toList.flatMap { case sampleId =>

      listChunks( blastChunksS3Prefix(sampleId) )
        .zipWithIndex
        .map { case (chunkS3Obj, n) =>

          DataMapping(sampleId, assignDataProcessing(parameters))(
            remoteInput = Map(
              data.blastChunk -> S3Resource(chunkS3Obj)
            ),
            remoteOutput = Map(
              data.lcaChunk -> S3Resource(lcaAssignS3Prefix(sampleId) / s"${sampleId}.lca.${n}.csv"),
              data.bbhChunk -> S3Resource(bbhAssignS3Prefix(sampleId) / s"${sampleId}.bbh.${n}.csv")
            )
          )
        }
    }

    lazy val merge = inputSamples.keys.toList.map { case sampleId =>

      DataMapping(sampleId, mergeDataProcessing)(
        remoteInput = Map(
          data.blastChunksFolder -> S3Resource(blastChunksS3Prefix(sampleId)),
          data.blastNoHitsFolder -> S3Resource(blastNoHitsS3Prefix(sampleId)),
          data.lcaChunksFolder   -> S3Resource(lcaAssignS3Prefix(sampleId)),
          data.bbhChunksFolder   -> S3Resource(bbhAssignS3Prefix(sampleId))
        ),
        remoteOutput = Map(
          data.blastResult -> S3Resource(outputS3Folder(sampleId, "merge") / s"${sampleId}.blast.csv"),
          data.blastNoHits -> S3Resource(outputS3Folder(sampleId, "merge") / s"${sampleId}.no-hits.fa"),
          data.lcaCSV      -> S3Resource(outputS3Folder(sampleId, "merge") / s"${sampleId}.lca.csv"),
          data.bbhCSV      -> S3Resource(outputS3Folder(sampleId, "merge") / s"${sampleId}.bbh.csv")
        )
      )
    }

    lazy val count: List[DataMapping[countDataProcessing.type]] = merge.map { case mergeDM =>
      val sampleId = mergeDM.label

      def outputFor(d: FileData): (FileData, S3Resource) = {
        d -> S3Resource(outputS3Folder(sampleId, "count") / s"${sampleId}.${d.baseName}.csv")
      }

      DataMapping(sampleId, countDataProcessing)(
        remoteInput = Map(
          lookup(data.lcaCSV, mergeDM.remoteOutput),
          lookup(data.bbhCSV, mergeDM.remoteOutput)
        ),
        remoteOutput = Map(
          outputFor(data.lca.direct.absolute),
          outputFor(data.lca.accum.absolute),
          outputFor(data.lca.direct.relative),
          outputFor(data.lca.accum.relative),
          outputFor(data.bbh.direct.absolute),
          outputFor(data.bbh.accum.absolute),
          outputFor(data.bbh.direct.relative),
          outputFor(data.bbh.accum.relative)
        )
      )
    }
  }

  case object splitLoquat  extends Loquat(splitConfig,  splitDataProcessing(parameters))(dataMappings.split)
  case object blastLoquat  extends Loquat(blastConfig,  blastDataProcessing(parameters))(dataMappings.blast)
  case object assignLoquat extends Loquat(assignConfig, assignDataProcessing(parameters))(dataMappings.assign)
  case object mergeLoquat  extends Loquat(mergeConfig,  mergeDataProcessing)(dataMappings.merge)
  case object countLoquat  extends Loquat(countConfig,  countDataProcessing)(dataMappings.count)
}


/* With the constructor it is just easier to bind the Parameters type member */
abstract class MG7Pipeline[P <: AnyMG7Parameters](
  val parameters: P
) extends AnyMG7Pipeline {
  type Parameters = P

  /* The rest of the members can be defined inside */
}

/* This kind of pipeline adds the Flash data preprocessing step */
trait AnyFlashMG7Pipeline extends AnyMG7Pipeline {

  val pairedInputs: Map[SampleID, (S3Resource, S3Resource)]

  val flashParameters: AnyFlashParameters
  val flashConfig: AnyFlashConfig

  lazy val flashDataMappings = pairedInputs.toList.map { case (sampleId, (reads1S3Resource, reads2S3Resource)) =>

    DataMapping(sampleId, flashDataProcessing(flashParameters))(
      remoteInput = Map(
        data.pairedReads1 -> reads1S3Resource,
        data.pairedReads2 -> reads2S3Resource
      ),
      remoteOutput = Map(
        data.mergedReads    -> S3Resource(outputS3Folder(sampleId, "flash") / s"${sampleId}.merged.fastq"),
        data.pair1NotMerged -> S3Resource(outputS3Folder(sampleId, "flash") / s"${sampleId}.pair1.not-merged.fastq"),
        data.pair2NotMerged -> S3Resource(outputS3Folder(sampleId, "flash") / s"${sampleId}.pair2.not-merged.fastq"),
        data.flashHistogram -> S3Resource(outputS3Folder(sampleId, "flash") / s"${sampleId}.hist")
      )
    )
  }

  /* This is the input of the base pipeline derived from the output of Flash */
  final lazy val sampleInputs: Map[SampleID, S3Resource] = flashDataMappings.map { flashDM =>
    flashDM.label -> flashDM.remoteOutput(data.mergedReads)
  }.toMap

  case object flashLoquat extends Loquat(flashConfig, flashDataProcessing(flashParameters))(flashDataMappings)
}
