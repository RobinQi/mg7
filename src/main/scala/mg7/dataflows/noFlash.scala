package ohnosequences.mg7.dataflows

import ohnosequences.mg7._, loquats._

import ohnosequences.datasets._

import ohnosequences.cosas._, types._, klists._

import ohnosequences.loquat._

import ohnosequences.statika._

import ohnosequences.awstools.ec2._, InstanceType._
import ohnosequences.awstools.s3._
import ohnosequences.awstools.autoscaling._
import ohnosequences.awstools.regions.Region._
import com.amazonaws.auth._, profile._

/* ## No-flash Dataflow

  No-flash dataflow does the same as the full one *but* the first flash-based merging step. See AnyFullDataflow.
*/
trait AnyNoFlashDataflow extends AnyDataflow {

  val splitInputs: Map[SampleID, S3Resource]

  lazy val splitDataMappings = splitInputs.toList.map { case (sampleId, readsS3Resource) =>

    DataMapping(sampleId, splitDataProcessing(params))(
      remoteInput = Map(
        data.mergedReads -> readsS3Resource
      ),
      remoteOutput = Map(
        data.fastaChunks -> S3Resource(params.outputS3Folder(sampleId, "split"))
      )
    )
  }

  private def instanceS3client(): S3 = S3.create(
    new AWSCredentialsProviderChain(
      new InstanceProfileCredentialsProvider(),
      new ProfileCredentialsProvider()
    )
  )

  private def listChunks(s3prefix: AnyS3Address): List[S3Object] = {
    instanceS3client().listObjects(s3prefix.bucket, s3prefix.key)
  }

  // These prefixes will be used several times, so they factored in methods:
  private def blastChunksS3Prefix(sampleId: String): S3Folder = params.outputS3Folder(sampleId, "blast") / "chunks" /
  private def blastNoHitsS3Prefix(sampleId: String): S3Folder = params.outputS3Folder(sampleId, "blast") / "no-hits" /

  // Here we generate tasks/data mappings for the blast loquat one per each S3 object generated by the split loquat
  lazy val blastDataMappings = splitDataMappings.flatMap { splitDM =>
    val sampleId = splitDM.label

    listChunks( splitDM.remoteOutput(data.fastaChunks).resource )
      .zipWithIndex
      .map { case (chunkS3Obj, n) =>

        DataMapping(s"${sampleId}.${n}", blastDataProcessing(params))(
          remoteInput = Map(
            data.fastaChunk -> S3Resource(chunkS3Obj)
          ),
          remoteOutput = Map(
            data.blastChunk  -> S3Resource(blastChunksS3Prefix(sampleId) / s"blast.${n}.csv"),
            data.noHitsChunk -> S3Resource(blastNoHitsS3Prefix(sampleId) / s"no-hits.${n}.fa")
          )
        )
      }
  }

  // These prefixes will be used several times, so they factored in methods:
  private def lcaAssignS3Prefix(sampleId: String): S3Folder = params.outputS3Folder(sampleId, "assign") / "lca" /
  private def bbhAssignS3Prefix(sampleId: String): S3Folder = params.outputS3Folder(sampleId, "assign") / "bbh" /

  lazy val assignDataMappings = splitInputs.keys.toList.flatMap { case sampleId =>

    listChunks( blastChunksS3Prefix(sampleId) )
      .zipWithIndex
      .map { case (chunkS3Obj, n) =>

        DataMapping(sampleId, assignDataProcessing(params))(
          remoteInput = Map(
            data.blastChunk -> S3Resource(chunkS3Obj)
          ),
          remoteOutput = Map(
            data.lcaChunk -> S3Resource(lcaAssignS3Prefix(sampleId) / s"${sampleId}.lca.${n}.csv"),
            data.bbhChunk -> S3Resource(bbhAssignS3Prefix(sampleId) / s"${sampleId}.bbh.${n}.csv")
          )
        )
      }
  }

  lazy val mergeDataMappings = splitInputs.keys.toList.map { case sampleId =>

    DataMapping(sampleId, mergeDataProcessing)(
      remoteInput = Map(
        data.blastChunksFolder -> S3Resource(blastChunksS3Prefix(sampleId)),
        data.blastNoHitsFolder -> S3Resource(blastNoHitsS3Prefix(sampleId)),
        data.lcaChunksFolder   -> S3Resource(lcaAssignS3Prefix(sampleId)),
        data.bbhChunksFolder   -> S3Resource(bbhAssignS3Prefix(sampleId))
      ),
      remoteOutput = Map(
        data.blastResult -> S3Resource(params.outputS3Folder(sampleId, "merge") / s"${sampleId}.blast.csv"),
        data.blastNoHits -> S3Resource(params.outputS3Folder(sampleId, "merge") / s"${sampleId}.no-hits.fa"),
        data.lcaCSV      -> S3Resource(params.outputS3Folder(sampleId, "merge") / s"${sampleId}.lca.csv"),
        data.bbhCSV      -> S3Resource(params.outputS3Folder(sampleId, "merge") / s"${sampleId}.bbh.csv")
      )
    )
  }

}

case class NoFlashDataflow[P <: AnyMG7Parameters](val params: P)(val splitInputs: Map[SampleID, S3Resource])
extends AnyNoFlashDataflow {

  type Params = P
}
